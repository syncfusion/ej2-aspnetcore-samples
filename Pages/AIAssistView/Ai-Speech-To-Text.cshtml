@page
@model EJ2CoreSampleBrowser_NET8.Pages.AIAssistView.Ai_Speech_To_TextModel
@using Syncfusion.EJ2
@using Syncfusion.EJ2.InteractiveChat
@using System.Text.Json;

@section ControlsSection {
    <div class="control-section">
        <div class="integration-speechtotext-section">
            <ejs-aiassistview id="aiAssistView"
                              bannerTemplate="#bannerContent"
                              footerTemplate="#footerContent"
                              promptRequest="onPromptRequest"
                              stopRespondingClick="stopRespondingClick"
                              created="onCreated"
                          >
                <e-aiassistview-toolbarsettings items="@Model.Items" itemClicked="toolbarItemClicked"></e-aiassistview-toolbarsettings>
                <e-aiassistview-prompttoolbarsettings itemClicked="promptToolbarItemClicked"></e-aiassistview-prompttoolbarsettings>
            </ejs-aiassistview>
        </div>
    </div>
}

@section PreScripts {
    <script src="https://cdn.jsdelivr.net/npm/marked@latest/marked.min.js"></script>
    <script>
        var assistObj = null;
        var speechToTextObj;
        var stopStreaming = false;
        // Azure OpenAI configuration
        const azureOpenAIApiKey = 'Your_AzureOpenAIApiKey'; // Replace with your key
        const azureOpenAIEndpoint = 'Your_AzureOpenAIEndpoint'; // Replace with your endpoint
        const azureOpenAIApiVersion = '2024-07-01-preview'; // Replace to match your resource
        const azureDeploymentName = 'Your_AzureDeploymentName'; // Replace with your deployment name
        

        function onCreated() {
            assistObj = ej.base.getComponent(document.getElementById("aiAssistView"), "aiassistview");
            // Initialize Speech-to-Text component
            speechToTextObj = new ej.inputs.SpeechToText({
                transcriptChanged: onTranscriptChange,
                onStop: onListeningStop,
                created: Created,
                cssClass: 'e-flat'
            });
            speechToTextObj.appendTo('#speechToText');
        }

        function promptToolbarItemClicked(args) {
            if (args.item.iconCss === "e-icons e-assist-edit") {
                const assistviewFooter = document.querySelector('#assistview-footer');
                assistviewFooter.innerHTML = assistObj.prompts[args.dataIndex].prompt;
                toggleButtons();
            }
        }

        function toolbarItemClicked(args) {
            if (args.item.iconCss === 'e-icons e-refresh') {
                assistObj.prompts = [];
                assistObj.promptSuggestions = suggestions;
                stopStreaming = true;
            }
        }

        async function streamResponse(response) {
            let streamedResponseText = '';
            if (response) {
                await new Promise(resolve => setTimeout(resolve, 300));
                let i = 0;
                while (i < response.length && !stopStreaming) {
                    streamedResponseText += response[i];
                    i++;
                    assistObj.addPromptResponse(marked.parse(streamedResponseText), false);
                    assistObj.scrollToBottom();
                    await new Promise(resolve => setTimeout(resolve, 10));
                }
            }
            return streamedResponseText;
        }

        async function onPromptRequest(args) {
            stopStreaming = false;
            try {
                const response = await fetch(
                    `${azureOpenAIEndpoint.replace(/\/$/, '')}/openai/deployments/${encodeURIComponent(azureDeploymentName)}/chat/completions?api-version=${encodeURIComponent(azureOpenAIApiVersion)}`,
                    {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                            'api-key': azureOpenAIApiKey
                        },
                        body: JSON.stringify({
                            messages: [{ role: 'user', content: args.prompt || 'Hi' }],
                            temperature: 0.7,
                            max_tokens: 200
                        })
                    }
                );
                const data = await response.json().catch(() => ({}));
                if (!response.ok) {
                    const errorMsg = data?.error?.message || `HTTP ${response.status} ${response.statusText}`;
                    throw new Error(errorMsg);
                }
                const fullResponse = data?.choices?.[0]?.message?.content?.trim() || 'No response received.';
                const streamedText = await streamResponse(fullResponse);
                if (!stopStreaming) {
                    assistObj.addPromptResponse(marked.parse(streamedText), true);
                }
                // Update prompt suggestions
                assistObj.promptSuggestions = suggestions;
            } catch (error) {
                assistObj.addPromptResponse('⚠️ Something went wrong while connecting to the OpenAI service. Please check your API key.', true);
                stopStreaming = true;
                toggleButtons();
            }
        }

        function onListeningStop() {
            toggleButtons();
        }

        function onTranscriptChange(args) {
            document.querySelector('#assistview-footer').innerText = args.transcript;
        }

        function Created() {
            let assistviewFooter = document.querySelector('#assistview-footer');
            let sendButton = document.querySelector('#assistview-sendButton');

            sendButton.addEventListener('click', sendIconClicked);
            assistviewFooter.addEventListener('input', toggleButtons);

            assistviewFooter.addEventListener('keydown', function (e) {
                if (e.key === 'Enter' && !e.shiftKey) {
                    sendIconClicked();
                    e.preventDefault();
                }
            });
            toggleButtons();
        }

        function toggleButtons() {
            let assistviewFooter = document.querySelector('#assistview-footer');
            let sendButton = document.querySelector('#assistview-sendButton');
            let speechButton = document.querySelector('#speechToText');

            let hasText = assistviewFooter.innerText.trim() !== '';
            sendButton.classList.toggle('visible', hasText);
            speechButton.classList.toggle('visible', !hasText);

            if (!hasText && (assistviewFooter.innerHTML === '<br>' || !assistviewFooter.innerHTML.trim())) {
                assistviewFooter.innerHTML = '';
            }
        }

        function sendIconClicked() {
            var assistviewFooter = document.querySelector('#assistview-footer');
            assistObj.executePrompt(assistviewFooter.innerText);
            assistviewFooter.innerText = '';
        }

        function stopRespondingClick() {
            stopStreaming = true;
        }
    </script>

    <script id="bannerContent" type="text/x-jsrender">
        <div class="banner-content">
            <div class="e-icons e-listen-icon"></div>
            <h3>Speech To Text</h3>
            <i>Click the below mic-button to convert your voice to text.</i>
        </div>
    </script>
    <script id="footerContent" type="text/x-jsrender">
        <div class="e-footer-wrapper">
            <div id="assistview-footer" class="content-editor" contenteditable="true" placeholder="Click to speak or start typing..."></div>
            <div class="option-container">
                <button id="speechToText"></button>
                <button id="assistview-sendButton" class="e-assist-send e-icons" role="button"></button>
            </div>
        </div>
    </script>
}

<style>
    .integration-speechtotext-section {
        height: 550px;
        width: 550px;
        margin: 0 auto;
    }

        .integration-speechtotext-section .e-view-container {
            margin: auto;
        }

        .integration-speechtotext-section .e-banner-view {
            margin-left: 0;
        }

        .integration-speechtotext-section .banner-content .e-listen-icon:before {
            font-size: 35px;
        }

        .integration-speechtotext-section .banner-content {
            display: flex;
            flex-direction: column;
            justify-content: center;
            height: 330px;
            text-align: center;
        }

        .integration-speechtotext-section #assistview-sendButton:not(.e-assist-stop) {
            width: 40px;
            height: 40px;
            font-size: 20px;
            border: none;
            background: none;
            cursor: pointer;
        }

        .e-bigger .integration-speechtotext-section #assistview-sendButton:not(.e-assist-stop) {
            width: 52px;
            height: 52px;
            font-size: 24px;
        }

        .integration-speechtotext-section #assistview-sendButton.e-assist-stop {
            width: 32px;
            height: 32px;
            margin: 4px;
            border: none;
            cursor: pointer;
        }

        .e-bigger .integration-speechtotext-section #assistview-sendButton.e-assist-stop {
            width: 40px;
            height: 40px;
            margin: 6px;
        }

            .integration-speechtotext-section #speechToText.visible,
            .integration-speechtotext-section #assistview-sendButton.visible {
                display: inline-block;
            }

        .integration-speechtotext-section #speechToText,
        .integration-speechtotext-section #assistview-sendButton {
            display: none;
        }

        .integration-speechtotext-section #speechToText {
            box-shadow: unset;
            background: unset;
            border: none;
            color: #555555;
        }

    body[class*="dark"] .integration-speechtotext-section #speechToText,
    body[class*="high"] .integration-speechtotext-section #speechToText {
        color: #fff;
    }
    @@media only screen and (max-width: 750px) {
        .integration-speechtotext-section

    {
        width: 100%;
    }

    }

    .integration-speechtotext-section .e-footer-wrapper {
        display: flex;
        border: 1px solid #c1c1c1;
        padding: 5px 5px 5px 10px;
        border-radius: 10px;
    }

    .integration-speechtotext-section .content-editor {
        width: 100%;
        overflow-y: auto;
        font-size: 14px;
        min-height: 25px;
        max-height: 200px;
        padding: 10px;
    }

        .integration-speechtotext-section .content-editor[contentEditable='true']:empty:before {
            content: attr(placeholder);
            font-style: italic;
            font-weight: 200;
        }

    .integration-speechtotext-section .option-container {
        align-self: flex-end;
    }
</style>

@section Meta {
    <meta name="description" content="This example demonstrates the integration of Speech-to-Text functionality with the ASP.NET Core AIAssistView control, using Azure OpenAI for responses. Explore here for more details." />
}

@section ActionDescription {
    <div id="action-description">
        <p>
            This sample demonstrates the integration of <code>Speech-to-Text</code> functionality with the AI AssistView component. It allows users to convert spoken input into text using the device's microphone and the browser's <code>SpeechRecognition</code> API, with responses powered by Azure OpenAI.
        </p>
    </div>
}

@section Description {
    <div id="description">
        <p>
            In this example, the AI AssistView component is integrated with the <code>SpeechToText</code> component to enable voice-based interaction with Azure OpenAI service.
        </p>
        <p>
            The sample demonstrates the following features:
        </p>
        <ul>
            <li>
                The <code>SpeechToText</code> component captures voice input and transcribes it into text, which is then passed to the AI AssistView for generating contextual responses using Azure OpenAI.
            </li>
            <li>
                The <code>footerTemplate</code> includes a content-editable area and a microphone button for initiating voice input.
            </li>
            <li>
                The <code>toolbarSettings</code> adds a right-aligned <code>Refresh</code> button to clear previous prompts.
            </li>
            <li>
                Responses are streamed dynamically using the <code>addPromptResponse</code> method for a real-time experience.
            </li>
            <li>
                Markdown content in the response is rendered using the <code>Marked</code> plugin.
            </li>
            <li>
                The <code>promptSuggestions</code> provides AI prompt suggestions to guide user interactions.
            </li>
        </ul>
    </div>
}